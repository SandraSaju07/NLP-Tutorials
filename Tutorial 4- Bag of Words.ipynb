{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6928a61-aa6b-4206-862c-7d41d3d2624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7577f881-ef02-4da6-9615-db033047fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "statistical natural language processing (NLP) for English written in the Python programming language. It supports \n",
    "classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.[4] It was developed \n",
    "by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.\n",
    "[5] NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying \n",
    "concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.[7]\n",
    "\n",
    "NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, \n",
    "cognitive science, artificial intelligence, information retrieval, and machine learning.[8] NLTK has been used \n",
    "successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research \n",
    "systems. There are 32 universities in the US and 25 countries using NLTK in their courses.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277a3aba-fe1b-4fb6-aa4c-9f96c5af517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96c1ee20-138f-4ea8-96dd-008f01a3c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20586095-d8b0-4db2-bd8a-99c82c6ab221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \\nstatistical natural language processing (NLP) for English written in the Python programming language.',\n",
       " 'It supports \\nclassification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.',\n",
       " '[4] It was developed \\nby Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.',\n",
       " '[5] NLTK includes graphical demonstrations and sample data.',\n",
       " 'It is accompanied by a book that explains the underlying \\nconcepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.',\n",
       " '[7]\\n\\nNLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, \\ncognitive science, artificial intelligence, information retrieval, and machine learning.',\n",
       " '[8] NLTK has been used \\nsuccessfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research \\nsystems.',\n",
       " 'There are 32 universities in the US and 25 countries using NLTK in their courses.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd945429-752b-4fbb-a3da-37bc060d54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [stemmer.stem(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7725880f-d86e-4d34-aa77-1c6abd7ed1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag toolkit commonli nltk suit librari program symbol statist natur languag process nlp english written python program languag',\n",
       " 'support classif token stem tag pars semant reason function',\n",
       " 'develop steven bird edward loper depart comput inform scienc univers pennsylvania',\n",
       " 'nltk includ graphic demonstr sampl data',\n",
       " 'accompani book explain underli concept behind languag process task support toolkit plu cookbook',\n",
       " 'nltk intend support research teach nlp close relat area includ empir linguist cognit scienc artifici intellig inform retriev machin learn',\n",
       " 'nltk use success teach tool individu studi tool platform prototyp build research system',\n",
       " 'univers us countri use nltk cours']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0afeeaa-4134-41de-b7ed-c974445f3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4ccfb9-134d-4423-bd16-c8eb6d423437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language toolkit commonly nltk suite library program symbolic statistical natural language processing nlp english written python programming language',\n",
       " 'support classification tokenization stemming tagging parsing semantic reasoning functionality',\n",
       " 'developed steven bird edward loper department computer information science university pennsylvania',\n",
       " 'nltk includes graphical demonstration sample data',\n",
       " 'accompanied book explains underlying concept behind language processing task supported toolkit plus cookbook',\n",
       " 'nltk intended support research teaching nlp closely related area including empirical linguistics cognitive science artificial intelligence information retrieval machine learning',\n",
       " 'nltk used successfully teaching tool individual study tool platform prototyping building research system',\n",
       " 'university u country using nltk course']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70b4bad9-4dc8-4dac-b035-5b9ae59e07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "120ebf9c-7fed-44f6-9fff-b3672d88fe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "053d321f-a879-4d62-bcad-d076fd7d42d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a006c0af-047d-4a0b-a117-5e97b7db4a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f444e357-276f-4cf6-b62c-8277b5c5c5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26e701-df2a-4d1b-b903-7610864d2b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
